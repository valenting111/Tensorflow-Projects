{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Copy of Logs dictionary.ipynb","provenance":[{"file_id":"1jepIhnWcguzdltyGkufBsf_Toz6wbhK_","timestamp":1609783212675}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pv7Br1sXjlt0"},"source":["# Using the logs dictionary\n","\n","In this reading, we will learn how to take advantage of the `logs` dictionary in Keras to define our own callbacks and check the progress of a model."]},{"cell_type":"code","metadata":{"id":"K42BNa23jlt5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609783867704,"user_tz":-60,"elapsed":2001,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}},"outputId":"e497f480-4507-429a-b3ec-0dcb7b09822c"},"source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["2.4.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8D1mxYv2jlt6"},"source":["The `logs` dictionary stores the loss value, along with all of the metrics we are using at the end of a batch or epoch.\n","\n","We can incorporate information from the `logs` dictionary into our own custom callbacks.\n","\n","Let's see this in action in the context of a model we will construct and fit to the `sklearn` diabetes dataset that we have been using in this module.\n","\n","Let's first import the dataset, and split it into the training and test sets."]},{"cell_type":"code","metadata":{"id":"9cbyMmv-jlt6","executionInfo":{"status":"ok","timestamp":1609783868368,"user_tz":-60,"elapsed":2659,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}}},"source":["# Load the diabetes dataset\n","\n","from sklearn.datasets import load_diabetes\n","\n","diabetes_dataset = load_diabetes()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"_WAug6QEjlt6","executionInfo":{"status":"ok","timestamp":1609783868370,"user_tz":-60,"elapsed":2658,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}}},"source":["# Save the input and target variables\n","\n","from sklearn.model_selection import train_test_split\n","\n","data = diabetes_dataset['data']\n","targets = diabetes_dataset['target']"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpSV_4u9jlt7","executionInfo":{"status":"ok","timestamp":1609783868371,"user_tz":-60,"elapsed":2658,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}}},"source":["# Split the data set into training and test sets\n","\n","train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Nv8MXr1jlt7"},"source":["Now we construct our model."]},{"cell_type":"code","metadata":{"id":"7Ay9uaRUjlt7","executionInfo":{"status":"ok","timestamp":1609783896766,"user_tz":-60,"elapsed":6442,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}}},"source":["# Build the model\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","model = tf.keras.Sequential([\n","    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n","    Dense(64,activation='relu'),\n","    tf.keras.layers.BatchNormalization(),\n","    Dense(64, activation='relu'),\n","    Dense(64, activation='relu'),\n","    Dense(1)        \n","])"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jUifomTrjlt7"},"source":["We now compile the model, with\n","* Mean squared error as the loss function,\n","* the Adam optimizer, and \n","* Mean absolute error (`mae`) as a metric."]},{"cell_type":"code","metadata":{"id":"q0NOe0pwjlt8","executionInfo":{"status":"ok","timestamp":1609783899018,"user_tz":-60,"elapsed":731,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}}},"source":["# Compile the model\n","    \n","model.compile(loss='mse', optimizer=\"adam\", metrics=['mae'])"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xiCNDvOFjlt8"},"source":["### Defining a custom callback\n","\n","Now we define our custom callback using the `logs` dictionary to access the loss and metric values."]},{"cell_type":"code","metadata":{"id":"Kagem0Pjjlt8","executionInfo":{"status":"ok","timestamp":1609784145975,"user_tz":-60,"elapsed":922,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}}},"source":["# Create the custom callback\n","\n","class LossAndMetricCallback(tf.keras.callbacks.Callback):\n","\n","    # Print the loss after every second batch in the training set\n","    def on_train_batch_end(self, batch, logs=None):\n","        if batch %2 ==0:\n","            print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n","    \n","    # Print the loss after each batch in the test set\n","    def on_test_batch_end(self, batch, logs=None):\n","        print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n","\n","    # Print the loss and mean absolute error after each epoch\n","    def on_epoch_end(self, epoch, logs=None):\n","        print('Epoch {}: Average loss is {:7.2f}, mean absolute error is {:7.2f}.'.format(epoch, logs['loss'], logs['mae']))\n","    \n","    # Notify the user when prediction has finished on each batch\n","    def on_predict_batch_end(self,batch, logs=None):\n","        print(\"Finished prediction on batch {}!\".format(batch))"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"90QAYYN8jlt8"},"source":["We now fit the model to the data, and specify that we would like to use our custom callback `LossAndMetricCallback()`."]},{"cell_type":"code","metadata":{"id":"So_eCTHGjlt9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609784157550,"user_tz":-60,"elapsed":3423,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}},"outputId":"0293f9cb-0e12-49a9-e361-72302749a262"},"source":["# Train the model\n","\n","history = model.fit(train_data, train_targets, epochs=20,\n","                    batch_size=100, callbacks=[LossAndMetricCallback()], verbose=False)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\n"," After batch 0, the loss is 31937.51.\n","\n"," After batch 2, the loss is 28150.56.\n","Epoch 0: Average loss is 29148.29, mean absolute error is  152.60.\n","\n"," After batch 0, the loss is 28034.39.\n","\n"," After batch 2, the loss is 28197.91.\n","Epoch 1: Average loss is 29017.70, mean absolute error is  152.21.\n","\n"," After batch 0, the loss is 30407.95.\n","\n"," After batch 2, the loss is 28199.87.\n","Epoch 2: Average loss is 28831.17, mean absolute error is  151.64.\n","\n"," After batch 0, the loss is 29160.07.\n","\n"," After batch 2, the loss is 28483.59.\n","Epoch 3: Average loss is 28558.87, mean absolute error is  150.82.\n","\n"," After batch 0, the loss is 28196.43.\n","\n"," After batch 2, the loss is 28470.44.\n","Epoch 4: Average loss is 28160.49, mean absolute error is  149.62.\n","\n"," After batch 0, the loss is 27049.41.\n","\n"," After batch 2, the loss is 27376.75.\n","Epoch 5: Average loss is 27627.96, mean absolute error is  148.03.\n","\n"," After batch 0, the loss is 28581.09.\n","\n"," After batch 2, the loss is 27802.42.\n","Epoch 6: Average loss is 26942.37, mean absolute error is  145.93.\n","\n"," After batch 0, the loss is 25150.86.\n","\n"," After batch 2, the loss is 27243.71.\n","Epoch 7: Average loss is 26059.40, mean absolute error is  143.15.\n","\n"," After batch 0, the loss is 22338.69.\n","\n"," After batch 2, the loss is 24687.28.\n","Epoch 8: Average loss is 24924.02, mean absolute error is  139.49.\n","\n"," After batch 0, the loss is 26342.90.\n","\n"," After batch 2, the loss is 24146.47.\n","Epoch 9: Average loss is 23553.18, mean absolute error is  134.85.\n","\n"," After batch 0, the loss is 24093.97.\n","\n"," After batch 2, the loss is 23526.20.\n","Epoch 10: Average loss is 21919.77, mean absolute error is  129.02.\n","\n"," After batch 0, the loss is 19315.34.\n","\n"," After batch 2, the loss is 20275.07.\n","Epoch 11: Average loss is 19954.13, mean absolute error is  121.78.\n","\n"," After batch 0, the loss is 15052.58.\n","\n"," After batch 2, the loss is 17927.97.\n","Epoch 12: Average loss is 17814.96, mean absolute error is  113.64.\n","\n"," After batch 0, the loss is 13492.98.\n","\n"," After batch 2, the loss is 15833.16.\n","Epoch 13: Average loss is 15437.36, mean absolute error is  103.55.\n","\n"," After batch 0, the loss is 13421.25.\n","\n"," After batch 2, the loss is 13350.78.\n","Epoch 14: Average loss is 13117.70, mean absolute error is   93.22.\n","\n"," After batch 0, the loss is 12430.67.\n","\n"," After batch 2, the loss is 11593.16.\n","Epoch 15: Average loss is 11027.54, mean absolute error is   83.37.\n","\n"," After batch 0, the loss is 10281.55.\n","\n"," After batch 2, the loss is 8854.59.\n","Epoch 16: Average loss is 9136.06, mean absolute error is   75.42.\n","\n"," After batch 0, the loss is 8673.39.\n","\n"," After batch 2, the loss is 7998.90.\n","Epoch 17: Average loss is 7545.30, mean absolute error is   67.60.\n","\n"," After batch 0, the loss is 6961.90.\n","\n"," After batch 2, the loss is 7046.95.\n","Epoch 18: Average loss is 6774.83, mean absolute error is   64.24.\n","\n"," After batch 0, the loss is 7026.21.\n","\n"," After batch 2, the loss is 6807.49.\n","Epoch 19: Average loss is 5958.19, mean absolute error is   60.44.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"COq4SOE6jlt9"},"source":["We can also use our callback in the `evaluate` function..."]},{"cell_type":"code","metadata":{"id":"Wnr7i-urjlt9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609784175318,"user_tz":-60,"elapsed":1866,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}},"outputId":"534be66b-017e-4187-f6ed-85c60095128c"},"source":["# Evaluate the model\n","\n","model_eval = model.evaluate(test_data, test_targets, batch_size=10, \n","                            callbacks=[LossAndMetricCallback()], verbose=False)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["\n"," After batch 0, the loss is 11190.75.\n","\n"," After batch 1, the loss is 16393.88.\n","\n"," After batch 2, the loss is 21462.74.\n","\n"," After batch 3, the loss is 19202.09.\n","\n"," After batch 4, the loss is 17844.18.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hl8TmBSEjlt9"},"source":["...And also the `predict` function."]},{"cell_type":"code","metadata":{"id":"1O0MwkwMjlt9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609784300633,"user_tz":-60,"elapsed":978,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}},"outputId":"22eb372e-f5c4-4588-e855-1456cb32cd6c"},"source":["# Get predictions from the model\n","\n","model_pred = model.predict(test_data, batch_size=10,\n","                           callbacks=[LossAndMetricCallback()], verbose=False)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Finished prediction on batch 0!\n","Finished prediction on batch 1!\n","Finished prediction on batch 2!\n","Finished prediction on batch 3!\n","Finished prediction on batch 4!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5Nv9qcTPjlt-"},"source":["### Application - learning rate scheduler\n","Let's now look at a more sophisticated custom callback. \n","\n","We are going to define a callback to change the learning rate of the optimiser of a model during training. We will do this by specifying the epochs and new learning rates where we would like it to be changed.\n","\n","First we define the auxillary function that returns the learning rate for each epoch based on our schedule."]},{"cell_type":"code","metadata":{"id":"xrACCnT2jlt-","executionInfo":{"status":"ok","timestamp":1609784509949,"user_tz":-60,"elapsed":623,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}}},"source":["# Define the learning rate schedule. The tuples below are (start_epoch, new_learning_rate)\n","\n","lr_schedule = [\n","    (4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)\n","]\n","\n","def get_new_epoch_lr(epoch, lr):\n","    # Checks to see if the input epoch is listed in the learning rate schedule \n","    # and if so, returns index in lr_schedule\n","    epoch_in_sched = [i for i in range(len(lr_schedule)) if lr_schedule[i][0]==int(epoch)]\n","    if len(epoch_in_sched)>0:\n","        # If it is, return the learning rate corresponding to the epoch\n","        return lr_schedule[epoch_in_sched[0]][1]\n","    else:\n","        # Otherwise, return the existing learning rate\n","        return lr"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yRe15Onljlt-"},"source":["Let's now define the callback itself."]},{"cell_type":"code","metadata":{"id":"BDfxXktWjlt-","executionInfo":{"status":"ok","timestamp":1609784585742,"user_tz":-60,"elapsed":557,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}}},"source":["# Define the custom callback\n","\n","class LRScheduler(tf.keras.callbacks.Callback):\n","    \n","    def __init__(self, new_lr):\n","        super(LRScheduler, self).__init__()\n","        # Add the new learning rate function to our callback\n","        self.new_lr = new_lr\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        # Make sure that the optimizer we have chosen has a learning rate, and raise an error if not\n","        if not hasattr(self.model.optimizer, 'lr'):\n","              raise ValueError('Error: Optimizer does not have a learning rate.')\n","                \n","        # Get the current learning rate\n","        curr_rate = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n","        \n","        # Call the auxillary function to get the scheduled learning rate for the current epoch\n","        scheduled_rate = self.new_lr(epoch, curr_rate)\n","\n","        # Set the learning rate to the scheduled learning rate\n","        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n","        print('Learning rate for epoch {} is {:7.3f}'.format(epoch, scheduled_rate))"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qYXhxh7rjlt-"},"source":["Let's now train the model again with our new callback. "]},{"cell_type":"code","metadata":{"id":"XCcsYgWXjlt_","executionInfo":{"status":"ok","timestamp":1609784588268,"user_tz":-60,"elapsed":642,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}}},"source":["# Build the same model as before\n","\n","new_model = tf.keras.Sequential([\n","    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n","    Dense(64,activation='relu'),\n","    tf.keras.layers.BatchNormalization(),\n","    Dense(64, activation='relu'),\n","    Dense(64, activation='relu'),\n","    Dense(1)        \n","])"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJ8_OLVNjlt_","executionInfo":{"status":"ok","timestamp":1609784592095,"user_tz":-60,"elapsed":701,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}}},"source":["# Compile the model\n","\n","new_model.compile(loss='mse',\n","                optimizer=\"adam\",\n","                metrics=['mae', 'mse'])"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSMsE5Dhjlt_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609784595350,"user_tz":-60,"elapsed":1409,"user":{"displayName":"Valentin Gourmet","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipXSkYHgIA7qUUkUpEqVgy4Uupcf67DVY3NWjUqQ=s64","userId":"13252817155483159617"}},"outputId":"d524a585-6c85-4d03-fed9-9fc6e9f68977"},"source":["# Fit the model with our learning rate scheduler callback\n","\n","new_history = new_model.fit(train_data, train_targets, epochs=20,\n","                            batch_size=100, callbacks=[LRScheduler(get_new_epoch_lr)], verbose=False)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Learning rate for epoch 0 is   0.001\n","Learning rate for epoch 1 is   0.001\n","Learning rate for epoch 2 is   0.001\n","Learning rate for epoch 3 is   0.001\n","Learning rate for epoch 4 is   0.030\n","Learning rate for epoch 5 is   0.030\n","Learning rate for epoch 6 is   0.030\n","Learning rate for epoch 7 is   0.020\n","Learning rate for epoch 8 is   0.020\n","Learning rate for epoch 9 is   0.020\n","Learning rate for epoch 10 is   0.020\n","Learning rate for epoch 11 is   0.005\n","Learning rate for epoch 12 is   0.005\n","Learning rate for epoch 13 is   0.005\n","Learning rate for epoch 14 is   0.005\n","Learning rate for epoch 15 is   0.007\n","Learning rate for epoch 16 is   0.007\n","Learning rate for epoch 17 is   0.007\n","Learning rate for epoch 18 is   0.007\n","Learning rate for epoch 19 is   0.007\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WY4B9Pj5jluA"},"source":["### Further reading and resources\n","* https://www.tensorflow.org/guide/keras/custom_callback\n","* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback"]}]}